{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1 Configuration\n",
    "* Get DB configuration variables from the `.env` file\n",
    "* Initialize the connection to the DB"
   ],
   "id": "a3330a97256e09fc"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-06T18:54:56.484216519Z",
     "start_time": "2026-01-06T18:54:56.399555538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Setup DB\n",
    "load_dotenv()\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "connection_str = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "engine = create_engine(connection_str) # From SQLAlchemy: The engine manages the communication and translation between Python and PostgreSQL\n",
    "\n",
    "print(\"Starting ETL Process...\")"
   ],
   "id": "273991890b623d59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ETL Process...\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# (Optional)\n",
    "* Reset the database if the script has been run before\n",
    "* Try/Except in case the script is run for the first time"
   ],
   "id": "d4dd5f14322dfc5d"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-06T18:55:17.523957538Z",
     "start_time": "2026-01-06T18:55:17.296827331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Similar to a try-except-finally block\n",
    "with engine.connect() as conn:\n",
    "    print(\"Cleaning old data...\")\n",
    "    try: # Try/except if script is run for the first time\n",
    "        # Deletes all tables\n",
    "        # Cascade: also remove dependent object to a removed table (foreign keys, tables)\n",
    "        # From SQLAlchemy: text to execute as a SQL statement\n",
    "        conn.execute(text(\"\"\"\n",
    "            DROP TABLE IF EXISTS\n",
    "                fact_sales,\n",
    "                dim_weather,\n",
    "                dim_customers,\n",
    "                dim_articles,\n",
    "                dim_region,\n",
    "                dim_customer_junk,\n",
    "                dim_customer_outrigger,\n",
    "                dim_product_type,\n",
    "                dim_graphical_appearance,\n",
    "                dim_color,\n",
    "                dim_time\n",
    "            CASCADE;\n",
    "        \"\"\"))\n",
    "        conn.commit()\n",
    "        print(\"Old data cleared.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Cleanup Error: {e}\")"
   ],
   "id": "8348dfae3a837ec0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning old data...\n",
      "Old data cleared.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2 SQL Table Definition",
   "id": "840944dfc532fef6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T18:55:22.368311287Z",
     "start_time": "2026-01-06T18:55:22.280824406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ddl_statements = \"\"\"\n",
    "-- 1. Region Dimension\n",
    "CREATE TABLE IF NOT EXISTS dim_region (\n",
    "    postal_code VARCHAR(64) PRIMARY KEY,\n",
    "    region_name VARCHAR(100)\n",
    ");\n",
    "\n",
    "-- 2. Customer Junk Dimension (Active / Club Status)\n",
    "-- SERIAL: auto-incrementing integer\n",
    "CREATE TABLE IF NOT EXISTS dim_customer_junk (\n",
    "    junk_key SERIAL PRIMARY KEY,\n",
    "    active NUMERIC(2,1),\n",
    "    club_member_status VARCHAR(50)\n",
    ");\n",
    "\n",
    "-- 3. Customer Outrigger (FN / Fashion News)\n",
    "CREATE TABLE IF NOT EXISTS dim_customer_outrigger (\n",
    "    o_key SERIAL PRIMARY KEY,\n",
    "    fn NUMERIC(2,1),\n",
    "    fn_freq VARCHAR(50)\n",
    ");\n",
    "\n",
    "-- 4. Customer Dimension\n",
    "CREATE TABLE IF NOT EXISTS dim_customers (\n",
    "    customer_id VARCHAR(64) PRIMARY KEY,\n",
    "    age INTEGER,\n",
    "    age_range VARCHAR(50),\n",
    "    junk_key INTEGER REFERENCES dim_customer_junk(junk_key),\n",
    "    o_key INTEGER REFERENCES dim_customer_outrigger(o_key),\n",
    "    postal_code VARCHAR(128) REFERENCES dim_region(postal_code)\n",
    ");\n",
    "\n",
    "-- 5. Product Type Dimension\n",
    "CREATE TABLE IF NOT EXISTS dim_product_type (\n",
    "    product_type_no INTEGER PRIMARY KEY,\n",
    "    product_type_name VARCHAR(255),\n",
    "    product_group_name VARCHAR(255)\n",
    ");\n",
    "\n",
    "-- 6. Graphical Appearance Dimension\n",
    "CREATE TABLE IF NOT EXISTS dim_graphical_appearance (\n",
    "    graph_appearance_no INTEGER PRIMARY KEY,\n",
    "    graph_appearance_name VARCHAR(255)\n",
    ");\n",
    "\n",
    "-- 7. Color Dimension\n",
    "CREATE TABLE IF NOT EXISTS dim_color (\n",
    "    color_group_code INTEGER PRIMARY KEY,\n",
    "    color_group_name VARCHAR(255),\n",
    "    perceived_color_value_name VARCHAR(255),\n",
    "    perceived_color_master_name VARCHAR(255)\n",
    ");\n",
    "\n",
    "-- 8. Article Dimension\n",
    "CREATE TABLE IF NOT EXISTS dim_articles (\n",
    "    article_id INTEGER PRIMARY KEY,\n",
    "    product_type_no INTEGER REFERENCES dim_product_type(product_type_no),\n",
    "    graph_appearance_no INTEGER REFERENCES dim_graphical_appearance(graph_appearance_no),\n",
    "    color_group_code INTEGER REFERENCES dim_color(color_group_code),\n",
    "    prod_code INTEGER,\n",
    "    prod_name VARCHAR(255)\n",
    ");\n",
    "\n",
    "-- 9. Time Dimension\n",
    "CREATE TABLE IF NOT EXISTS dim_time (\n",
    "    t_dat DATE PRIMARY KEY,\n",
    "    day INTEGER,\n",
    "    weekday INTEGER,\n",
    "    week INTEGER,\n",
    "    month INTEGER,\n",
    "    season VARCHAR(20)\n",
    ");\n",
    "\n",
    "-- 10. Weather Dimension\n",
    "CREATE TABLE IF NOT EXISTS dim_weather (\n",
    "    day DATE PRIMARY KEY REFERENCES dim_time(t_dat),\n",
    "    weather_code INTEGER,\n",
    "    description VARCHAR(255)\n",
    ");\n",
    "\n",
    "-- 11. Fact Table (Sales)\n",
    "CREATE TABLE IF NOT EXISTS fact_sales (\n",
    "    t_dat DATE REFERENCES dim_time(t_dat),\n",
    "    customer_id VARCHAR(64) REFERENCES dim_customers(customer_id),\n",
    "    article_id INTEGER REFERENCES dim_articles(article_id),\n",
    "    price NUMERIC(10,5),\n",
    "    sales_channel_id INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute DDL\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(ddl_statements))\n",
    "    conn.commit()\n",
    "\n",
    "print(\"Schema Checked/Created.\")"
   ],
   "id": "3f81dff5dcc52980",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Checked/Created.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3 Extract\n",
    "* Read in the CSV-files with Pandas into DataFrames\n",
    "* Clean the weather DataFrame a bit"
   ],
   "id": "a13f32bcd8f285bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T18:55:49.044889867Z",
     "start_time": "2026-01-06T18:55:29.589473234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use the package pandas to read in the csv files:\n",
    "df_trans = pd.read_csv('data/transactions.csv', parse_dates=['t_dat'])\n",
    "df_articles = pd.read_csv('data/articles.csv')\n",
    "df_customers = pd.read_csv('data/customers.csv')\n",
    "df_weather = pd.read_csv('data/open-meteo.csv', parse_dates=['day'])\n",
    "\n",
    "# strip() because weather had some leading or trailing whitespaces which caused an error\n",
    "df_weather.columns = df_weather.columns.str.strip()\n",
    "\n",
    "# Rename the second column to \"weather_code\"\n",
    "for col in df_weather.columns:\n",
    "    if 'code' in col.lower() and 'weather' in col.lower():\n",
    "        df_weather.rename(columns={col: 'weather_code'}, inplace=True)\n",
    "\n",
    "print(\"Files Loaded.\")"
   ],
   "id": "f76b26dad3d2d72e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files Loaded.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4 Transform & Load",
   "id": "fa340a0f9da3e5d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### a) Region Dimension\n",
    "* Get unique postal codes\n",
    "* Calculate the region name for every unique postal code\n",
    "* Create a new column in the DataFrame and save to the DB"
   ],
   "id": "9b6bfdbb25722e68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T18:56:09.106192976Z",
     "start_time": "2026-01-06T18:56:02.623937415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Logic: Map postal_code (Hex) to int MOD 10 = region_name\n",
    "\n",
    "# Dictionary for the region names\n",
    "region_names = {\n",
    "    1: 'Stockholm', 2: 'Södermanland / Östergötland', 3: 'Jönköping',\n",
    "    4: 'Skåne', 5: 'Kronoberg / Kalmar', 6: 'Värmland / Dalarna',\n",
    "    7: 'Gävleborg / Västernorrland', 8: 'Västerbotten / Norrbotten',\n",
    "    9: 'Blekinge', 0: 'Gotland'\n",
    "}\n",
    "\n",
    "# drop_duplicates because we only want to store each postal code once\n",
    "# reset_index to recalculate the indexes correctly\n",
    "unique_postals = df_customers[['postal_code']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def calc_region_name(p_code):\n",
    "    try:\n",
    "        r_idx = int(p_code, 16) % 10\n",
    "        return region_names.get(r_idx, 'Unknown')\n",
    "    except:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Create a new column in the DataFrame\n",
    "unique_postals['region_name'] = unique_postals['postal_code'].apply(calc_region_name)\n",
    "\n",
    "# Save in the DB\n",
    "unique_postals.to_sql('dim_region', engine, if_exists='append', index=False)\n",
    "print(f\"Loaded {len(unique_postals)} regions.\")\n"
   ],
   "id": "9d70a896f71a557e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 352899 regions.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### b) Product Type Dimension\n",
    "* Write the unique Product Types to the Dimension Table in the DB"
   ],
   "id": "835fd0fe422227a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T18:56:23.482527426Z",
     "start_time": "2026-01-06T18:56:23.330389122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Relevant columns:\n",
    "cols_prod = ['product_type_no', 'product_type_name', 'product_group_name']\n",
    "\n",
    "df_prod = df_articles[cols_prod].drop_duplicates('product_type_no')\n",
    "df_prod.to_sql('dim_product_type', engine, if_exists='append', index=False)"
   ],
   "id": "fe6b9561567d11d1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### c) Graphical Appearance Dimension\n",
    "* Same as in b) with the addition of renaming some columns"
   ],
   "id": "3aeb1968a9e07453"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T18:56:26.005743185Z",
     "start_time": "2026-01-06T18:56:25.889017553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cols_graph = ['graphical_appearance_no', 'graphical_appearance_name']\n",
    "df_graph = df_articles[cols_graph].drop_duplicates('graphical_appearance_no')\n",
    "\n",
    "# Rename the column to fit the names with defined in the DWH-Schema\n",
    "df_graph.rename(columns={'graphical_appearance_no': 'graph_appearance_no',\n",
    "                         'graphical_appearance_name': 'graph_appearance_name'}, inplace=True)\n",
    "df_graph.to_sql('dim_graphical_appearance', engine, if_exists='append', index=False)"
   ],
   "id": "23385ac7abd5371a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### d) Color Dimension\n",
    "* Exactly the same as c)"
   ],
   "id": "32d99e3b0e3a59c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T18:56:28.603534513Z",
     "start_time": "2026-01-06T18:56:28.530326664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cols_color = ['colour_group_code', 'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name']\n",
    "df_color = df_articles[cols_color].drop_duplicates('colour_group_code')\n",
    "\n",
    "# Rename the column to fit the names with defined in the DWH-Schema\n",
    "df_color.rename(columns={\n",
    "    'colour_group_code': 'color_group_code',\n",
    "    'colour_group_name': 'color_group_name',\n",
    "    'perceived_colour_value_name': 'perceived_color_value_name',\n",
    "    'perceived_colour_master_name': 'perceived_color_master_name'\n",
    "}, inplace=True)\n",
    "df_color.to_sql('dim_color', engine, if_exists='append', index=False)"
   ],
   "id": "5ef428586ba31420",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### e) Article Dimension\n",
    "* Same as c) again"
   ],
   "id": "40123500820ef7d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T18:56:33.921906965Z",
     "start_time": "2026-01-06T18:56:31.166496715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cols_art = ['article_id', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'product_code', 'prod_name']\n",
    "df_art = df_articles[cols_art].drop_duplicates('article_id')\n",
    "\n",
    "# Rename the column to fit the names with defined in the DWH-Schema\n",
    "df_art.rename(columns={\n",
    "    'graphical_appearance_no': 'graph_appearance_no',\n",
    "    'colour_group_code': 'color_group_code',\n",
    "    'product_code': 'prod_code'\n",
    "}, inplace=True)\n",
    "df_art.to_sql('dim_articles', engine, if_exists='append', index=False)"
   ],
   "id": "325b54174ed20b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "542"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### f) Customer Dimensions (Junk & Outrigger)\n",
    "* Create the junk and the outrigger dimension"
   ],
   "id": "e07e750b214fd716"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T18:56:37.091762287Z",
     "start_time": "2026-01-06T18:56:36.806332483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# f1) Junk: Active, club_member_status\n",
    "df_junk = df_customers[['Active', 'club_member_status']].drop_duplicates().reset_index(drop=True)\n",
    "df_junk['junk_key'] = df_junk.index + 1 # Index starts at 1\n",
    "# Rename the column to fit the names with defined in the DWH-Schema:\n",
    "df_junk.rename(columns={'Active': 'active'}, inplace=True)\n",
    "df_junk.to_sql('dim_customer_junk', engine, if_exists='append', index=False)\n",
    "\n",
    "# f2) Outrigger: FN, fashion_news_frequency\n",
    "df_outrigger = df_customers[['FN', 'fashion_news_frequency']].drop_duplicates().reset_index(drop=True)\n",
    "df_outrigger['o_key'] = df_outrigger.index + 1 # Index starts at 1\n",
    "# Rename the column to fit the names with defined in the DWH-Schema:\n",
    "df_outrigger.rename(columns={'FN': 'fn', 'fashion_news_frequency': 'fn_freq'}, inplace=True)\n",
    "df_outrigger.to_sql('dim_customer_outrigger', engine, if_exists='append', index=False)"
   ],
   "id": "da601aa393426424",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### g) Main Customer Dimension\n",
    "* Join the Junk and the Outrigger Dimension with the Customer Dimension"
   ],
   "id": "4485d14701b4c499"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T18:57:27.676290184Z",
     "start_time": "2026-01-06T18:56:41.203014666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Left Join to merge Junk and Outrigger\n",
    "df_c_m = df_customers.merge(df_junk.rename(columns={'active': 'Active'}), on=['Active', 'club_member_status'], how='left')\n",
    "df_c_m = df_c_m.merge(df_outrigger.rename(columns={'fn': 'FN', 'fn_freq': 'fashion_news_frequency'}), on=['FN', 'fashion_news_frequency'], how='left')\n",
    "\n",
    "# Calculate an age range\n",
    "def categorize_age(age):\n",
    "    # For NaN/Null values:\n",
    "    if pd.isna(age):\n",
    "        return 'Unknown'\n",
    "\n",
    "    # Assumptions:\n",
    "    if age < 25:\n",
    "        return 'Young Adult'\n",
    "    elif age < 45:\n",
    "        return 'Adult'\n",
    "    elif age < 65:\n",
    "        return 'Middle Aged'\n",
    "    else:\n",
    "        return 'Senior'\n",
    "\n",
    "# Map the age to an age range\n",
    "df_c_m['age_range'] = df_c_m['age'].apply(categorize_age)\n",
    "\n",
    "# Final DataFrame\n",
    "df_c_final = df_c_m[['customer_id', 'age', 'age_range', 'junk_key', 'o_key', 'postal_code']]\n",
    "df_c_final.to_sql('dim_customers', engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"Customers Loaded.\")"
   ],
   "id": "a426163adf25cf75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers Loaded.\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### h) Time Dimension\n",
    "* Save all unique dates from the transactions-table and the weather-table into a DataFrame\n",
    "* Calculate day, weekday, week, month and season"
   ],
   "id": "4531ba22844c9db9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T18:57:33.825317555Z",
     "start_time": "2026-01-06T18:57:33.705271051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dates = pd.DataFrame({'t_dat': pd.unique(np.concatenate((df_trans['t_dat'], df_weather['day']), 0))})\n",
    "dates['day'] = dates['t_dat'].dt.day\n",
    "dates['weekday'] = dates['t_dat'].dt.weekday # 0=Monday\n",
    "dates['week'] = dates['t_dat'].dt.isocalendar().week\n",
    "dates['month'] = dates['t_dat'].dt.month\n",
    "\n",
    "# Map month to season\n",
    "def get_season(m):\n",
    "    if m in [12, 1, 2]: return 'Winter'\n",
    "    elif m in [3, 4, 5]: return 'Spring'\n",
    "    elif m in [6, 7, 8]: return 'Summer'\n",
    "    else: return 'Autumn'\n",
    "\n",
    "dates['season'] = dates['month'].apply(get_season)\n",
    "\n",
    "dates.to_sql('dim_time', engine, if_exists='append', index=False)"
   ],
   "id": "35719e12b13e1586",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### i) Weather dimension",
   "id": "e4c8703b3a816ad4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T18:57:37.890036583Z",
     "start_time": "2026-01-06T18:57:37.809897601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Processing Weather...\")\n",
    "\n",
    "# Map weather code to weather description\n",
    "def get_weather_category(code):\n",
    "    try:\n",
    "        c = int(code)\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    if 0 <= c <= 19:\n",
    "        return \"No precipitation\"\n",
    "    elif 20 <= c <= 29:\n",
    "        return \"Precipitation but not at time of observation\"\n",
    "    elif 30 <= c <= 39:\n",
    "        return \"Sandstorm or Duststorm\"\n",
    "    elif 40 <= c <= 49:\n",
    "        return \"Fog or Ice Fog\"\n",
    "    elif 50 <= c <= 59:\n",
    "        return \"Drizzle\"\n",
    "    elif 60 <= c <= 69:\n",
    "        return \"Rain\"\n",
    "    elif 70 <= c <= 79:\n",
    "        return \"Solid precipitation not in showers\"\n",
    "    elif 80 <= c <= 99:\n",
    "        return \"Showery precipitation\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "df_weather['description'] = df_weather['weather_code'].apply(get_weather_category)\n",
    "\n",
    "df_weather_final = df_weather[['day', 'weather_code', 'description']]\n",
    "df_weather_final.to_sql('dim_weather', engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"Weather loaded with aggregated descriptions.\")"
   ],
   "id": "a8d4fbdb42c3e5ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Weather...\n",
      "Weather loaded with aggregated descriptions.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### j) Fact table\n",
    "* Create the fact table"
   ],
   "id": "9f01156d840c963a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-06T19:10:29.401194800Z",
     "start_time": "2026-01-06T18:57:44.032779402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_facts = df_trans[['t_dat', 'customer_id', 'article_id', 'price', 'sales_channel_id']]\n",
    "df_facts.to_sql('fact_sales', engine, if_exists='append', index=False)\n",
    "\n",
    "print(\"ETL Process Complete.\")"
   ],
   "id": "12a70501bcb5240c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL Process Complete.\n"
     ]
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
